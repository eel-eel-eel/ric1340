{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eel-eel-eel/ric1340/blob/main/ch06_02_exercise_answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a16rwiOqrDfz"
      },
      "source": [
        "# 固有表現抽出の練習問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-SdMDTFrRis"
      },
      "source": [
        "## データ準備・前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDcqy3jyha9I",
        "outputId": "4a6bc480-9296-4326-80a8-14834a7d97cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-08-15 12:30:29--  https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/chABSA-dataset.zip\n",
            "Resolving s3-ap-northeast-1.amazonaws.com (s3-ap-northeast-1.amazonaws.com)... 52.219.4.170\n",
            "Connecting to s3-ap-northeast-1.amazonaws.com (s3-ap-northeast-1.amazonaws.com)|52.219.4.170|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 722777 (706K) [application/zip]\n",
            "Saving to: ‘chABSA-dataset.zip’\n",
            "\n",
            "chABSA-dataset.zip  100%[===================>] 705.84K  1.52MB/s    in 0.5s    \n",
            "\n",
            "2022-08-15 12:30:30 (1.52 MB/s) - ‘chABSA-dataset.zip’ saved [722777/722777]\n",
            "\n",
            "Archive:  chABSA-dataset.zip\n",
            "   creating: chABSA-dataset/\n",
            "  inflating: chABSA-dataset/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/chABSA-dataset/\n",
            "  inflating: __MACOSX/chABSA-dataset/._.DS_Store  \n",
            " extracting: chABSA-dataset/.gitkeep  \n",
            "  inflating: chABSA-dataset/e00008_ann.json  \n",
            "  inflating: chABSA-dataset/e00017_ann.json  \n",
            "  inflating: chABSA-dataset/e00024_ann.json  \n",
            "  inflating: chABSA-dataset/e00026_ann.json  \n",
            "  inflating: chABSA-dataset/e00030_ann.json  \n",
            "  inflating: chABSA-dataset/e00033_ann.json  \n",
            "  inflating: chABSA-dataset/e00034_ann.json  \n",
            "  inflating: chABSA-dataset/e00035_ann.json  \n",
            "  inflating: chABSA-dataset/e00037_ann.json  \n",
            "  inflating: chABSA-dataset/e00051_ann.json  \n",
            "  inflating: chABSA-dataset/e00053_ann.json  \n",
            "  inflating: chABSA-dataset/e00058_ann.json  \n",
            "  inflating: chABSA-dataset/e00069_ann.json  \n",
            "  inflating: chABSA-dataset/e00091_ann.json  \n",
            "  inflating: chABSA-dataset/e00107_ann.json  \n",
            "  inflating: chABSA-dataset/e00114_ann.json  \n",
            "  inflating: chABSA-dataset/e00146_ann.json  \n",
            "  inflating: chABSA-dataset/e00168_ann.json  \n",
            "  inflating: chABSA-dataset/e00184_ann.json  \n",
            "  inflating: chABSA-dataset/e00194_ann.json  \n",
            "  inflating: chABSA-dataset/e00273_ann.json  \n",
            "  inflating: chABSA-dataset/e00308_ann.json  \n",
            "  inflating: chABSA-dataset/e00343_ann.json  \n",
            "  inflating: chABSA-dataset/e00354_ann.json  \n",
            "  inflating: chABSA-dataset/e00380_ann.json  \n",
            "  inflating: chABSA-dataset/e00406_ann.json  \n",
            "  inflating: chABSA-dataset/e00435_ann.json  \n",
            "  inflating: chABSA-dataset/e00457_ann.json  \n",
            "  inflating: chABSA-dataset/e00465_ann.json  \n",
            "  inflating: chABSA-dataset/e00501_ann.json  \n",
            "  inflating: chABSA-dataset/e00534_ann.json  \n",
            "  inflating: chABSA-dataset/e00541_ann.json  \n",
            "  inflating: chABSA-dataset/e00547_ann.json  \n",
            "  inflating: chABSA-dataset/e00563_ann.json  \n",
            "  inflating: chABSA-dataset/e00603_ann.json  \n",
            "  inflating: chABSA-dataset/e00686_ann.json  \n",
            "  inflating: chABSA-dataset/e00693_ann.json  \n",
            "  inflating: chABSA-dataset/e00694_ann.json  \n",
            "  inflating: chABSA-dataset/e00721_ann.json  \n",
            "  inflating: chABSA-dataset/e00772_ann.json  \n",
            "  inflating: chABSA-dataset/e00787_ann.json  \n",
            "  inflating: chABSA-dataset/e00810_ann.json  \n",
            "  inflating: chABSA-dataset/e00832_ann.json  \n",
            "  inflating: chABSA-dataset/e00838_ann.json  \n",
            "  inflating: chABSA-dataset/e00840_ann.json  \n",
            "  inflating: chABSA-dataset/e00842_ann.json  \n",
            "  inflating: chABSA-dataset/e00858_ann.json  \n",
            "  inflating: chABSA-dataset/e00877_ann.json  \n",
            "  inflating: chABSA-dataset/e00886_ann.json  \n",
            "  inflating: chABSA-dataset/e00909_ann.json  \n",
            "  inflating: chABSA-dataset/e00911_ann.json  \n",
            "  inflating: chABSA-dataset/e00939_ann.json  \n",
            "  inflating: chABSA-dataset/e00962_ann.json  \n",
            "  inflating: chABSA-dataset/e00976_ann.json  \n",
            "  inflating: chABSA-dataset/e01018_ann.json  \n",
            "  inflating: chABSA-dataset/e01043_ann.json  \n",
            "  inflating: chABSA-dataset/e01054_ann.json  \n",
            "  inflating: chABSA-dataset/e01097_ann.json  \n",
            "  inflating: chABSA-dataset/e01118_ann.json  \n",
            "  inflating: chABSA-dataset/e01151_ann.json  \n",
            "  inflating: chABSA-dataset/e01156_ann.json  \n",
            "  inflating: chABSA-dataset/e01173_ann.json  \n",
            "  inflating: chABSA-dataset/e01183_ann.json  \n",
            "  inflating: chABSA-dataset/e01197_ann.json  \n",
            "  inflating: chABSA-dataset/e01216_ann.json  \n",
            "  inflating: chABSA-dataset/e01230_ann.json  \n",
            "  inflating: chABSA-dataset/e01244_ann.json  \n",
            "  inflating: chABSA-dataset/e01249_ann.json  \n",
            "  inflating: chABSA-dataset/e01260_ann.json  \n",
            "  inflating: chABSA-dataset/e01334_ann.json  \n",
            "  inflating: chABSA-dataset/e01364_ann.json  \n",
            "  inflating: chABSA-dataset/e01398_ann.json  \n",
            "  inflating: chABSA-dataset/e01402_ann.json  \n",
            "  inflating: chABSA-dataset/e01425_ann.json  \n",
            "  inflating: chABSA-dataset/e01436_ann.json  \n",
            "  inflating: chABSA-dataset/e01462_ann.json  \n",
            "  inflating: chABSA-dataset/e01469_ann.json  \n",
            "  inflating: chABSA-dataset/e01506_ann.json  \n",
            "  inflating: chABSA-dataset/e01528_ann.json  \n",
            "  inflating: chABSA-dataset/e01529_ann.json  \n",
            "  inflating: chABSA-dataset/e01533_ann.json  \n",
            "  inflating: chABSA-dataset/e01546_ann.json  \n",
            "  inflating: chABSA-dataset/e01585_ann.json  \n",
            "  inflating: chABSA-dataset/e01620_ann.json  \n",
            "  inflating: chABSA-dataset/e01624_ann.json  \n",
            "  inflating: chABSA-dataset/e01629_ann.json  \n",
            "  inflating: chABSA-dataset/e01635_ann.json  \n",
            "  inflating: chABSA-dataset/e01703_ann.json  \n",
            "  inflating: chABSA-dataset/e01719_ann.json  \n",
            "  inflating: chABSA-dataset/e01731_ann.json  \n",
            "  inflating: chABSA-dataset/e01740_ann.json  \n",
            "  inflating: chABSA-dataset/e01743_ann.json  \n",
            "  inflating: chABSA-dataset/e01764_ann.json  \n",
            "  inflating: chABSA-dataset/e01794_ann.json  \n",
            "  inflating: chABSA-dataset/e01798_ann.json  \n",
            "  inflating: chABSA-dataset/e01813_ann.json  \n",
            "  inflating: chABSA-dataset/e01849_ann.json  \n",
            "  inflating: chABSA-dataset/e01862_ann.json  \n",
            "  inflating: chABSA-dataset/e01865_ann.json  \n",
            "  inflating: chABSA-dataset/e01903_ann.json  \n",
            "  inflating: chABSA-dataset/e01904_ann.json  \n",
            "  inflating: chABSA-dataset/e01933_ann.json  \n",
            "  inflating: chABSA-dataset/e01946_ann.json  \n",
            "  inflating: chABSA-dataset/e01968_ann.json  \n",
            "  inflating: chABSA-dataset/e01972_ann.json  \n",
            "  inflating: chABSA-dataset/e01974_ann.json  \n",
            "  inflating: chABSA-dataset/e01987_ann.json  \n",
            "  inflating: chABSA-dataset/e01992_ann.json  \n",
            "  inflating: chABSA-dataset/e02009_ann.json  \n",
            "  inflating: chABSA-dataset/e02049_ann.json  \n",
            "  inflating: chABSA-dataset/e02105_ann.json  \n",
            "  inflating: chABSA-dataset/e02150_ann.json  \n",
            "  inflating: chABSA-dataset/e02152_ann.json  \n",
            "  inflating: chABSA-dataset/e02214_ann.json  \n",
            "  inflating: chABSA-dataset/e02241_ann.json  \n",
            "  inflating: chABSA-dataset/e02246_ann.json  \n",
            "  inflating: chABSA-dataset/e02289_ann.json  \n",
            "  inflating: chABSA-dataset/e02353_ann.json  \n",
            "  inflating: chABSA-dataset/e02367_ann.json  \n",
            "  inflating: chABSA-dataset/e02380_ann.json  \n",
            "  inflating: chABSA-dataset/e02382_ann.json  \n",
            "  inflating: chABSA-dataset/e02390_ann.json  \n",
            "  inflating: chABSA-dataset/e02414_ann.json  \n",
            "  inflating: chABSA-dataset/e02423_ann.json  \n",
            "  inflating: chABSA-dataset/e02505_ann.json  \n",
            "  inflating: chABSA-dataset/e02525_ann.json  \n",
            "  inflating: chABSA-dataset/e02530_ann.json  \n",
            "  inflating: chABSA-dataset/e02544_ann.json  \n",
            "  inflating: chABSA-dataset/e02547_ann.json  \n",
            "  inflating: chABSA-dataset/e02563_ann.json  \n",
            "  inflating: chABSA-dataset/e02567_ann.json  \n",
            "  inflating: chABSA-dataset/e02608_ann.json  \n",
            "  inflating: chABSA-dataset/e02627_ann.json  \n",
            "  inflating: chABSA-dataset/e02632_ann.json  \n",
            "  inflating: chABSA-dataset/e02643_ann.json  \n",
            "  inflating: chABSA-dataset/e02673_ann.json  \n",
            "  inflating: chABSA-dataset/e02682_ann.json  \n",
            "  inflating: chABSA-dataset/e02732_ann.json  \n",
            "  inflating: chABSA-dataset/e02825_ann.json  \n",
            "  inflating: chABSA-dataset/e02837_ann.json  \n",
            "  inflating: chABSA-dataset/e02889_ann.json  \n",
            "  inflating: chABSA-dataset/e02905_ann.json  \n",
            "  inflating: chABSA-dataset/e02946_ann.json  \n",
            "  inflating: chABSA-dataset/e03128_ann.json  \n",
            "  inflating: chABSA-dataset/e03226_ann.json  \n",
            "  inflating: chABSA-dataset/e03236_ann.json  \n",
            "  inflating: chABSA-dataset/e03267_ann.json  \n",
            "  inflating: chABSA-dataset/e03275_ann.json  \n",
            "  inflating: chABSA-dataset/e03281_ann.json  \n",
            "  inflating: chABSA-dataset/e03367_ann.json  \n",
            "  inflating: chABSA-dataset/e03398_ann.json  \n",
            "  inflating: chABSA-dataset/e03401_ann.json  \n",
            "  inflating: chABSA-dataset/e03472_ann.json  \n",
            "  inflating: chABSA-dataset/e03505_ann.json  \n",
            "  inflating: chABSA-dataset/e03556_ann.json  \n",
            "  inflating: chABSA-dataset/e03566_ann.json  \n",
            "  inflating: chABSA-dataset/e03580_ann.json  \n",
            "  inflating: chABSA-dataset/e03582_ann.json  \n",
            "  inflating: chABSA-dataset/e03584_ann.json  \n",
            "  inflating: chABSA-dataset/e03601_ann.json  \n",
            "  inflating: chABSA-dataset/e03641_ann.json  \n",
            "  inflating: chABSA-dataset/e03693_ann.json  \n",
            "  inflating: chABSA-dataset/e03723_ann.json  \n",
            "  inflating: chABSA-dataset/e03784_ann.json  \n",
            "  inflating: chABSA-dataset/e03929_ann.json  \n",
            "  inflating: chABSA-dataset/e03991_ann.json  \n",
            "  inflating: chABSA-dataset/e04078_ann.json  \n",
            "  inflating: chABSA-dataset/e04091_ann.json  \n",
            "  inflating: chABSA-dataset/e04123_ann.json  \n",
            "  inflating: chABSA-dataset/e04136_ann.json  \n",
            "  inflating: chABSA-dataset/e04147_ann.json  \n",
            "  inflating: chABSA-dataset/e04191_ann.json  \n",
            "  inflating: chABSA-dataset/e04242_ann.json  \n",
            "  inflating: chABSA-dataset/e04273_ann.json  \n",
            "  inflating: chABSA-dataset/e04291_ann.json  \n",
            "  inflating: chABSA-dataset/e04298_ann.json  \n",
            "  inflating: chABSA-dataset/e04304_ann.json  \n",
            "  inflating: chABSA-dataset/e04319_ann.json  \n",
            "  inflating: chABSA-dataset/e04329_ann.json  \n",
            "  inflating: chABSA-dataset/e04331_ann.json  \n",
            "  inflating: chABSA-dataset/e04360_ann.json  \n",
            "  inflating: chABSA-dataset/e04503_ann.json  \n",
            "  inflating: chABSA-dataset/e04509_ann.json  \n",
            "  inflating: chABSA-dataset/e04727_ann.json  \n",
            "  inflating: chABSA-dataset/e04768_ann.json  \n",
            "  inflating: chABSA-dataset/e04844_ann.json  \n",
            "  inflating: chABSA-dataset/e04856_ann.json  \n",
            "  inflating: chABSA-dataset/e04858_ann.json  \n",
            "  inflating: chABSA-dataset/e04867_ann.json  \n",
            "  inflating: chABSA-dataset/e04877_ann.json  \n",
            "  inflating: chABSA-dataset/e04948_ann.json  \n",
            "  inflating: chABSA-dataset/e04976_ann.json  \n",
            "  inflating: chABSA-dataset/e04995_ann.json  \n",
            "  inflating: chABSA-dataset/e05011_ann.json  \n",
            "  inflating: chABSA-dataset/e05110_ann.json  \n",
            "  inflating: chABSA-dataset/e05145_ann.json  \n",
            "  inflating: chABSA-dataset/e05155_ann.json  \n",
            "  inflating: chABSA-dataset/e05167_ann.json  \n",
            "  inflating: chABSA-dataset/e05302_ann.json  \n",
            "  inflating: chABSA-dataset/e05319_ann.json  \n",
            "  inflating: chABSA-dataset/e05322_ann.json  \n",
            "  inflating: chABSA-dataset/e05346_ann.json  \n",
            "  inflating: chABSA-dataset/e05452_ann.json  \n",
            "  inflating: chABSA-dataset/e05469_ann.json  \n",
            "  inflating: chABSA-dataset/e05480_ann.json  \n",
            "  inflating: chABSA-dataset/e05593_ann.json  \n",
            "  inflating: chABSA-dataset/e05629_ann.json  \n",
            "  inflating: chABSA-dataset/e05714_ann.json  \n",
            "  inflating: chABSA-dataset/e05737_ann.json  \n",
            "  inflating: chABSA-dataset/e07801_ann.json  \n",
            "  inflating: chABSA-dataset/e21200_ann.json  \n",
            "  inflating: chABSA-dataset/e21261_ann.json  \n",
            "  inflating: chABSA-dataset/e23818_ann.json  \n",
            "  inflating: chABSA-dataset/e25665_ann.json  \n",
            "  inflating: chABSA-dataset/e26332_ann.json  \n",
            "  inflating: chABSA-dataset/e26443_ann.json  \n",
            "  inflating: chABSA-dataset/e26454_ann.json  \n",
            "  inflating: chABSA-dataset/e26713_ann.json  \n",
            "  inflating: chABSA-dataset/e26914_ann.json  \n",
            "  inflating: chABSA-dataset/e27050_ann.json  \n",
            "  inflating: chABSA-dataset/e27633_ann.json  \n",
            "  inflating: chABSA-dataset/e27759_ann.json  \n",
            "  inflating: chABSA-dataset/e30066_ann.json  \n",
            "  inflating: chABSA-dataset/e30085_ann.json  \n",
            "  inflating: chABSA-dataset/e30479_ann.json  \n",
            "  inflating: chABSA-dataset/e30746_ann.json  \n",
            "  inflating: chABSA-dataset/e32161_ann.json  \n",
            "  inflating: chABSA-dataset/e32189_ann.json  \n",
            "  inflating: chABSA-dataset/e32458_ann.json  \n",
            "  inflating: chABSA-dataset/e33009_ann.json  \n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/chABSA-dataset.zip\n",
        "!unzip chABSA-dataset.zip\n",
        "!mkdir -p /content/drive/MyDrive/bert/6_2_chABSA\n",
        "!cp -r chABSA-dataset /content/drive/MyDrive/bert/6_2_chABSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNQShcwrib5m",
        "outputId": "a4b7295a-a1c7-48c7-fd81-23dbc1dbc959"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['market#general', 'company#general', 'company#sales', 'company#profit', 'company#amount', 'company#price', 'company#cost', 'business#general', 'business#sales', 'business#profit', 'business#amount', 'business#price', 'business#cost', 'product#general', 'product#sales', 'product#profit', 'product#amount', 'product#price', 'product#cost']\n"
          ]
        }
      ],
      "source": [
        "label_kinds = []\n",
        "\n",
        "# make labels (exclude NULL and OOD)\n",
        "for e in [\"market\", \"company\", \"business\", \"product\"]:\n",
        "    for a in [\"general\", \"sales\", \"profit\", \"amount\", \"price\", \"cost\"]:\n",
        "        label_kinds.append(e + \"#\" + a)\n",
        "        if e in [\"market\"]:\n",
        "            break;\n",
        "\n",
        "print(label_kinds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-86NgkcglK0L",
        "outputId": "be34c7b0-7c88-425a-c3c3-5eeacbb40bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                            sentence  \\\n",
            "0  資源・金属素材関連では、世界的な政治の混乱などが実体経済へ波及し、資源需要が全般的に低迷した...   \n",
            "1  産機・建機関連では、企業収益や設備投資にやや改善傾向が見られたことから、民間企業向け各種ポン...   \n",
            "2  環境設備関連では、主力商品、関連商品とも堅調に推移し、水砕関連もほぼ計画のとおりとなったこと...   \n",
            "3  プラント・設備工事関連では、大口受注工事の完工などから、関連部門の売上高は45億１百万円とな...   \n",
            "4  化成品関連では、自動車関連及び一部電線業界が堅調に推移したものの、原油価格の低迷から販売価格...   \n",
            "\n",
            "                                            entities  \n",
            "0  [{'name': '資源・金属素材関連', 'span': [0, 9], 'type':...  \n",
            "1  [{'name': '産機・建機関連', 'span': [0, 7], 'type': '...  \n",
            "2  [{'name': '環境設備関連', 'span': [0, 6], 'type': 'b...  \n",
            "3  [{'name': 'プラント・設備工事関連', 'span': [0, 11], 'typ...  \n",
            "4  [{'name': '化成品関連', 'span': [0, 5], 'type': 'bu...  \n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import glob\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "\n",
        "dataset = []\n",
        "labels = []\n",
        "\n",
        "for f in glob.glob(\"/content/drive/MyDrive/bert/6_2_chABSA/chABSA-dataset/e*_ann.json\"):\n",
        "    with open(f, encoding=\"utf-8\") as j:\n",
        "        d = json.load(j)\n",
        "        for s in d[\"sentences\"]:\n",
        "            os = []\n",
        "            cs = []\n",
        "            ws = []\n",
        "            from_to = []\n",
        "            temp_dict = []\n",
        "            for o in s[\"opinions\"]:\n",
        "                category = o[\"category\"]\n",
        "                word = o[\"target\"]\n",
        "\n",
        "                if category in label_kinds and category not in cs:\n",
        "                  cs.append(category)\n",
        "                  category = re.sub(\"#\\D*\", \"\", o[\"category\"])\n",
        "                  temp_dict2 = dict(name=word, span=[o[\"from\"],o[\"to\"]],type=category)\n",
        "                  temp_dict.append(temp_dict2)\n",
        "\n",
        "            if len(cs) > 0:\n",
        "                dataset.append(\n",
        "                    {\"sentence\": s[\"sentence\"],\n",
        "                     \"entities\": temp_dict}\n",
        "                )\n",
        "\n",
        "\n",
        "dataset = pd.DataFrame(dataset)\n",
        "print(dataset.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii6xwcDZ4-A3",
        "outputId": "3f9a87df-4f49-428d-a5c9-fec0f067f93f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of                                                sentence  \\\n",
              "0     資源・金属素材関連では、世界的な政治の混乱などが実体経済へ波及し、資源需要が全般的に低迷した...   \n",
              "1     産機・建機関連では、企業収益や設備投資にやや改善傾向が見られたことから、民間企業向け各種ポン...   \n",
              "2     環境設備関連では、主力商品、関連商品とも堅調に推移し、水砕関連もほぼ計画のとおりとなったこと...   \n",
              "3     プラント・設備工事関連では、大口受注工事の完工などから、関連部門の売上高は45億１百万円とな...   \n",
              "4     化成品関連では、自動車関連及び一部電線業界が堅調に推移したものの、原油価格の低迷から販売価格...   \n",
              "...                                                 ...   \n",
              "1339  同事業の販売状況につきましては、自動車部品向けや、中国販売子会社での販売増により、販売数量は...   \n",
              "1340  電子部品向け金属粉の販売状況につきましては、スマートフォン関連需要に加え、軟磁性材向けの販売...   \n",
              "1341  粉末冶金向け金属粉の販売状況につきましては、自動車部品向けが好調な一方、熊本地震等の影響によ...   \n",
              "1342  セグメント利益は、電子部品向け金属粉の販売好調により、同186百万円増益（32.6％増益）の...   \n",
              "1343  不動産賃貸事業の当連結会計年度の売上高は23百万円（前年度比18.5%増収）、セグメント利益...   \n",
              "\n",
              "                                               entities  \n",
              "0     [{'name': '資源・金属素材関連', 'span': [0, 9], 'type':...  \n",
              "1     [{'name': '産機・建機関連', 'span': [0, 7], 'type': '...  \n",
              "2     [{'name': '環境設備関連', 'span': [0, 6], 'type': 'b...  \n",
              "3     [{'name': 'プラント・設備工事関連', 'span': [0, 11], 'typ...  \n",
              "4     [{'name': '化成品関連', 'span': [0, 5], 'type': 'bu...  \n",
              "...                                                 ...  \n",
              "1339  [{'name': '自動車部品向け', 'span': [16, 23], 'type':...  \n",
              "1340  [{'name': '金属粉', 'span': [6, 9], 'type': 'prod...  \n",
              "1341  [{'name': '金属粉', 'span': [6, 9], 'type': 'prod...  \n",
              "1342  [{'name': '金属粉', 'span': [15, 18], 'type': 'pr...  \n",
              "1343  [{'name': '不動産賃貸事業', 'span': [0, 7], 'type': '...  \n",
              "\n",
              "[1344 rows x 2 columns]>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAc9QN5qtTYI",
        "outputId": "6512cb14-8ebd-4729-c55d-4e5e43d16454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "最大文字数： 334\n"
          ]
        }
      ],
      "source": [
        "max_len = 0\n",
        "\n",
        "for i in range(dataset.shape[0]):\n",
        "  if max_len < len(dataset.loc[i, \"sentence\"]):\n",
        "    max_len = len(dataset.loc[i, \"sentence\"])\n",
        "print(\"最大文字数：\",max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlxPOQIwuwUh",
        "outputId": "f1ebf962-f597-4707-d16f-280c97cf7a4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers[ja]\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 8.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 73.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (3.7.1)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (4.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.8 MB/s \n",
            "\u001b[?25hCollecting fugashi>=1.0\n",
            "  Downloading fugashi-1.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (568 kB)\n",
            "\u001b[K     |████████████████████████████████| 568 kB 64.6 MB/s \n",
            "\u001b[?25hCollecting unidic-lite>=1.0.7\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4 MB 133 kB/s \n",
            "\u001b[?25hCollecting ipadic<2.0,>=1.0.0\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.4 MB 48.9 MB/s \n",
            "\u001b[?25hCollecting unidic>=1.0.2\n",
            "  Downloading unidic-1.1.0.tar.gz (7.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[ja]) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[ja]) (3.0.9)\n",
            "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from unidic>=1.0.2->transformers[ja]) (0.10.1)\n",
            "Collecting plac<2.0.0,>=1.1.3\n",
            "  Downloading plac-1.3.5-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[ja]) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[ja]) (3.8.1)\n",
            "Building wheels for collected packages: ipadic, unidic, unidic-lite\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=37c5b96a112f9ace130606bc2ff97e9fc70aa8f5d0eed603cc3cbc3e5a5539f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/8b/99/cf0d27191876637cd3639a560f93aa982d7855ce826c94348b\n",
            "  Building wheel for unidic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic: filename=unidic-1.1.0-py3-none-any.whl size=7426 sha256=ca09c30875008efccae1d0cda6a49105798697741deebff7c4377ef175990a20\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/4d/f1/170bb74b559ca338113c0315c9805e16dfd0a12411ec6b1122\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658836 sha256=7779ee9a3cc2cfd679c87d0b349c86e8a99f9dd65175a97a6afae7f6d7dbde8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/69/b1/112140b599f2b13f609d485a99e357ba68df194d2079c5b1a2\n",
            "Successfully built ipadic unidic unidic-lite\n",
            "Installing collected packages: pyyaml, tokenizers, plac, huggingface-hub, unidic-lite, unidic, transformers, ipadic, fugashi\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed fugashi-1.1.2 huggingface-hub-0.8.1 ipadic-1.0.0 plac-1.3.5 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1 unidic-1.1.0 unidic-lite-1.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[ja]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "4d7644e9b71d431f827a5c01730568d8",
            "10adad5c989f4a59a8e2f9c03a0fb75a",
            "d755c2410f284c8d981da9ae9c22867a"
          ]
        },
        "id": "b3WGfiZzy1-e",
        "outputId": "ff9599a5-9379-438f-cbd1-93f01997da05"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d7644e9b71d431f827a5c01730568d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading vocab.txt:   0%|          | 0.00/252k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10adad5c989f4a59a8e2f9c03a0fb75a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/110 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d755c2410f284c8d981da9ae9c22867a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/479 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BertJapaneseTokenizer\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2ek_TZjazJjr"
      },
      "outputs": [],
      "source": [
        "list_text = [dataset.loc[i,\"sentence\"] for i in range(dataset.shape[0])]\n",
        "list_tokens = [tokenizer.tokenize(text) for text in list_text]\n",
        "list_entities = [dataset.loc[i,\"entities\"] for i in range(dataset.shape[0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZACK8ZSIpRW9"
      },
      "outputs": [],
      "source": [
        "# cl-tohoku/bert-base-japanese-whole-word-maskingのモデルは最大512トークンまで対応しているが、\n",
        "# 学習時のGPUメモリ消費を抑えるため256としている\n",
        "n_token = 256\n",
        "\n",
        "# encode後のトークンは、特殊トークン（{CLS], {SEP]など）や特殊文字（##）が挿入されることに注意\n",
        "# spanで表されているラベルと文字数ベースで位置ずれを起こすため、BIO形式に変換する際に補正する必要がある\n",
        "list_text_id =  [tokenizer.encode(text, truncation=True, padding='max_length', max_length=n_token) for text in list_text]\n",
        "list_tokens = [tokenizer.convert_ids_to_tokens(encode) for encode in list_text_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Usxs8p0ypchT"
      },
      "outputs": [],
      "source": [
        "def is_in_span(idx, span):\n",
        "  return span[0] <= idx and idx < span[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L8fv4e9UpfdU"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "list_bio =[]\n",
        "\n",
        "label2id = defaultdict(lambda :len(label2id))\n",
        "_ = label2id[\"O\"]\n",
        "\n",
        "for text, tokens, entities in zip(list_text, list_tokens, list_entities):\n",
        "  bio = [\"O\"] * len(tokens)\n",
        "\n",
        "  for entity in entities:\n",
        "    cnt = 0\n",
        "    begin_flg = True\n",
        "    label = entity[\"type\"]\n",
        "\n",
        "    for i, tok in enumerate(tokens):\n",
        "      if tok == \"[CLS]\" or tok == \"[SEP]\" or tok == \"[PAD]\":\n",
        "        continue\n",
        "      elif is_in_span(cnt, entity[\"span\"]):\n",
        "        if begin_flg:\n",
        "          bio[i] = f\"B-{label}\"\n",
        "          begin_flg = False\n",
        "          _ = label2id[f\"B-{label}\"]\n",
        "          _ = label2id[f\"I-{label}\"]\n",
        "        else:\n",
        "          bio[i] = f\"I-{label}\"\n",
        "\n",
        "      cnt += len(tok.replace(\"##\", \"\"))\n",
        "\n",
        "  list_bio.append(bio)\n",
        "\n",
        "id2label = {v:k for k,v in label2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Cn995lVFphZD"
      },
      "outputs": [],
      "source": [
        "list_bio_id = [[label2id[label] for label in bio] for bio in list_bio]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Uz0sC8PplO_",
        "outputId": "cb0c8cbd-9bcf-40c6-9379-b3092589c054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS]\tO\n",
            "資源\tB-business\n",
            "・\tI-business\n",
            "金属\tI-business\n",
            "素材\tI-business\n",
            "関連\tI-business\n",
            "で\tO\n",
            "は\tO\n",
            "、\tO\n",
            "世界\tO\n",
            "的\tO\n",
            "な\tO\n",
            "政治\tO\n",
            "の\tO\n",
            "混乱\tO\n",
            "など\tO\n",
            "が\tO\n",
            "実体\tO\n",
            "経済\tO\n",
            "へ\tO\n",
            "波及\tO\n",
            "し\tO\n",
            "、\tO\n",
            "資源\tO\n",
            "需要\tO\n",
            "が\tO\n",
            "全般\tO\n",
            "的\tO\n",
            "に\tO\n",
            "低迷\tO\n",
            "し\tO\n",
            "た\tO\n",
            "こと\tO\n",
            "から\tO\n",
            "、\tO\n",
            "関連\tO\n",
            "部門\tO\n",
            "の\tO\n",
            "売上\tO\n",
            "高\tO\n",
            "は\tO\n",
            "93\tO\n",
            "億\tO\n",
            "3\tO\n",
            "百\tO\n",
            "万\tO\n",
            "円\tO\n",
            "と\tO\n",
            "なり\tO\n",
            "、\tO\n",
            "前年\tO\n",
            "同期\tO\n",
            "と\tO\n",
            "比べ\tO\n",
            "13\tO\n",
            "億\tO\n",
            "46\tO\n",
            "百\tO\n",
            "万\tO\n",
            "円\tO\n",
            "(\tO\n",
            "##△\tO\n",
            "12\tO\n",
            ".\tO\n",
            "6\tO\n",
            "%)\tO\n",
            "の\tO\n",
            "減\tO\n",
            "##収\tO\n",
            "と\tO\n",
            "なり\tO\n",
            "まし\tO\n",
            "た\tO\n",
            "[SEP]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n",
            "[PAD]\tO\n"
          ]
        }
      ],
      "source": [
        "# BIO形式に変換できていることの確認\n",
        "for dec, bio, _ in zip(list_tokens[0], list_bio[0], range(100)):\n",
        "  print(f\"{dec}\\t{bio}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCA6Bzh8pmke",
        "outputId": "918fdff1-c034-4b96-c8cb-bcb2d9787525"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1344"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(list_bio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywwgsHaTprVk",
        "outputId": "bdcd3e15-c92c-449a-ab78-d6977f547de9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 使用デバイスにGPUを設定\n",
        "# 以下のような出力が出ていれば正常に設定ができている\n",
        "# device(type='cuda', index=0)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OO_lzFbJptyN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "  def __init__(self, texts_id, bios_id, is_test=False):\n",
        "    self.texts_id = texts_id\n",
        "    self.bios_id = bios_id\n",
        "    self.is_test = is_test\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    data = {'input_ids': torch.tensor(self.texts_id[idx], device=device)}\n",
        "    if not self.is_test:\n",
        "      data['label'] = torch.tensor(self.bios_id[idx], device=device)\n",
        "    return data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.bios_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFWd5Ibt8hKl"
      },
      "source": [
        "データ分割"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AQs6G_uqUv-1"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.6\n",
        "val_ratio = 0.2\n",
        "test_ratio = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nQrPoUJtqWk1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "list_text_id_train, list_text_id_test, list_bio_id_train, list_bio_id_test = \\\n",
        "    train_test_split(list_text_id, list_bio_id, test_size=test_ratio, random_state=42)\n",
        "list_text_id_train, list_text_id_valid, list_bio_id_train, list_bio_id_valid = \\\n",
        "    train_test_split(list_text_id_train, list_bio_id_train, test_size=val_ratio/(test_ratio+train_ratio), random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j5wrWt9E_Dj8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4UgFKEi_Ex4"
      },
      "source": [
        "データを.txt形式で保存しておく"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q34WIk8k-cyf"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/bert/6_2_chABSA/dataset/\n",
        "\n",
        "import pickle\n",
        "# データの保存\n",
        "f = open('/content/drive/MyDrive/bert/6_2_chABSA/dataset/list_text_id_train.txt', 'wb')\n",
        "pickle.dump(list_text_id_train, f)\n",
        "f = open('/content/drive/MyDrive/bert/6_2_chABSA/dataset/list_text_id_valid.txt', 'wb')\n",
        "pickle.dump(list_text_id_valid, f)\n",
        "f = open('/content/drive/MyDrive/bert/6_2_chABSA/dataset/list_text_id_test.txt', 'wb')\n",
        "pickle.dump(list_text_id_test, f)\n",
        "\n",
        "f = open('/content/drive/MyDrive/bert/6_2_chABSA/dataset/list_bio_id_train.txt', 'wb')\n",
        "pickle.dump(list_bio_id_train, f)\n",
        "f = open('/content/drive/MyDrive/bert/6_2_chABSA/dataset/list_bio_id_valid.txt', 'wb')\n",
        "pickle.dump(list_bio_id_valid, f)\n",
        "f = open('/content/drive/MyDrive/bert/6_2_chABSA/dataset/list_bio_id_test.txt', 'wb')\n",
        "pickle.dump(list_bio_id_test, f)\n",
        "\n",
        "\n",
        "# データの読み込み\n",
        "# f = open('/content/drive/MyDrive/bert/6_2_chABSA/dataset/list_text_id_train.txt', 'rb')\n",
        "# list_text_id_train = pickle.load(f)\n",
        "# f = open('/content/drive/MyDrive/bert/6_2_chABSA/dataset/list_text_id_valid.txt', 'rb')\n",
        "# list_text_id_valid = pickle.load(f)\n",
        "# f = open('/content/drive/MyDrive/bert/6_2_chABSA/dataset/list_text_id_test.txt', 'rb')\n",
        "# list_text_id_test = pickle.load(f)\n",
        "\n",
        "# f = open('/content/drive/MyDrive/bert/6_2_chABSA/dataset/list_bio_id_train.txt', 'rb')\n",
        "# list_bio_id_train = pickle.load(f)\n",
        "# f = open('/content/drive/MyDrive/bert/6_2_chABSA/dataset/list_bio_id_valid.txt', 'rb')\n",
        "# list_bio_id_valid = pickle.load(f)\n",
        "# f = open('/content/drive/MyDrive/bert/6_2_chABSA/dataset/list_bio_id_test.txt', 'rb')\n",
        "# list_bio_id_test = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "u64V3kaT-7Lm",
        "outputId": "c9f92dc2-a374-4bbe-e51a-bea711d68743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "学習データ数： 806\n",
            "検証データ数： 269\n",
            "テストデータ数： 269\n"
          ]
        }
      ],
      "source": [
        "ds_train = NERDataset(list_text_id_train, list_bio_id_train)\n",
        "ds_valid = NERDataset(list_text_id_valid, list_bio_id_valid)\n",
        "ds_test = NERDataset(list_text_id_test, list_bio_id_test, is_test=True)\n",
        "\n",
        "print(\"学習データ数：\", len(ds_train))\n",
        "print(\"検証データ数：\", len(ds_valid))\n",
        "print(\"テストデータ数：\", len(ds_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b-qI6np0qd5x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iObKJUcqgxq"
      },
      "source": [
        "## 固有表現抽出モデルの作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrucKYNICaeT"
      },
      "source": [
        "## 解答1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b22e2bd68329451d9af4a959a73fb3b4"
          ]
        },
        "id": "1wHSE1x8rp1o",
        "outputId": "3a3b65a9-ba67-47e3-e0f2-b08c4e1bc5af"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b22e2bd68329451d9af4a959a73fb3b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/424M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Q. BertForTokenClassificationを使って、事前学習済みモデルを読み込みましょう\n",
        "\n",
        "from transformers import BertForTokenClassification\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path='cl-tohoku/bert-base-japanese-whole-word-masking',\n",
        "    id2label=id2label,\n",
        "    label2id=label2id)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I94D2CBjrqTp"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/bert/6_2_chABSA/results\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CRLXTYpCxwk"
      },
      "source": [
        "\n",
        "## 解答2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UgaIA-Zyru3Z"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "\n",
        "# Q. training_configとtrainerを定義しましょう\n",
        "\n",
        "training_config = TrainingArguments(\n",
        "  output_dir='/content/drive/MyDrive/bert/6_2_chABSA/results',\n",
        "  num_train_epochs=20,\n",
        "  per_device_train_batch_size=batch_size,\n",
        "  per_device_eval_batch_size=batch_size,\n",
        "  warmup_steps=500,\n",
        "  weight_decay=0.2,\n",
        "  save_steps=500,\n",
        "  do_eval=True,\n",
        "  eval_steps=100\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_config,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=ds_train,\n",
        "    eval_dataset=ds_valid\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b5V8BlDDNol"
      },
      "source": [
        "## 解答3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "id": "y2-__Fq8sHsk",
        "outputId": "31d117a3-f97b-426b-aefb-36cf59887514"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 806\n",
            "  Num Epochs = 20\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2020\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2020' max='2020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2020/2020 13:18, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.121100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.007600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.001500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-500\n",
            "Configuration saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-500/special_tokens_map.json\n",
            "Saving model checkpoint to /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-1000\n",
            "Configuration saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-1000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-1000/special_tokens_map.json\n",
            "Saving model checkpoint to /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-1500\n",
            "Configuration saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-1500/config.json\n",
            "Model weights saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-1500/special_tokens_map.json\n",
            "Saving model checkpoint to /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-2000\n",
            "Configuration saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-2000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/bert/6_2_chABSA/results/checkpoint-2000/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2020, training_loss=0.032334402688239505, metrics={'train_runtime': 801.3531, 'train_samples_per_second': 20.116, 'train_steps_per_second': 2.521, 'total_flos': 2106185144094720.0, 'train_loss': 0.032334402688239505, 'epoch': 20.0})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Q. trainerを使って、学習しましょう\n",
        "\n",
        "trainer.train() # この行に学習を行うコードを実装しましょう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ztg24L5MsPe9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbTzyS2YsT2B"
      },
      "source": [
        "## テストデータの推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "We0j9qW0spnG",
        "outputId": "8ae2131d-d834-4f1d-ed11-947ed185378a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval[gpu]\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 695 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval[gpu]) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval[gpu]) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=60a54009d6d46ba5e1489e705b02d804ca86fe8828e2920b9ec014a242a417ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "# 固有表現抽出の評価用ライブラリをインストール\n",
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-LOfv6cD3HL"
      },
      "source": [
        "## 解答4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "Q40meD93snSv",
        "outputId": "a2762b7c-cffd-4cd2-d009-3de590424401"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 269\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Q. trainerを使って、推論結果をresult変数としましょう\n",
        "\n",
        "result = trainer.predict(ds_test) # この行にテストデータで推論を行うコードを実装しましょう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A4fuCjB3ssf8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "trues =  np.vectorize(lambda x:id2label[x])(ds_test.bios_id).tolist()\n",
        "\n",
        "preds_id = np.argmax(result.predictions, axis=2)\n",
        "preds = np.vectorize(lambda x:id2label[x])(preds_id).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jftXYJuQst9c",
        "outputId": "75762de1-556a-45f1-da4e-bcdf3e8fefe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6149341142020498\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    business       0.66      0.74      0.70       148\n",
            "     company       0.12      0.25      0.17         8\n",
            "      market       0.44      0.57      0.50        21\n",
            "     product       0.56      0.60      0.58       145\n",
            "\n",
            "   micro avg       0.58      0.65      0.61       322\n",
            "   macro avg       0.45      0.54      0.49       322\n",
            "weighted avg       0.59      0.65      0.62       322\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from seqeval.metrics import classification_report\n",
        "from seqeval.metrics import f1_score\n",
        "\n",
        "print(f1_score(trues, preds))\n",
        "\n",
        "print(classification_report(trues, preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLLkw_7OKuG1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}